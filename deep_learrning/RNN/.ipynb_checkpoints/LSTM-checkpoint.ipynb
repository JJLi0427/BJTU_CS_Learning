{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07629bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from datasets import *\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 启用CUDA加速"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780c2432",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_legacyLSTM(nn.Module):\n",
    "    def _init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.w_f = nn.Parameter(torch.rand(input_size, hidden_size))\n",
    "        self.u_f = nn.Parameter(torch.rand(hidden_size, hidden_size))\n",
    "        self.b_f = nn.Parameter(torch.zeros(hidden_size))\n",
    "\n",
    "        self.w_i = nn.Parameter(torch.rand(input_size, hidden_size))\n",
    "        self.u_i = nn.Parameter(torch.rand(hidden_size, hidden_size))\n",
    "        self.b_i = nn.Parameter(torch.zeros(hidden_size))\n",
    "\n",
    "        self.w_o = nn.Parameter(torch.rand(input_size, hidden_size))\n",
    "        self.u_o = nn.Parameter(torch.rand(hidden_size, hidden_size))\n",
    "        self.b_o = nn.Parameter(torch.zeros(hidden_size))\n",
    "\n",
    "        self.w_c = nn.Parameter(torch.rand(input_size, hidden_size))\n",
    "        self.u_c = nn.Parameter(torch.rand(hidden_size, hidden_size))\n",
    "        self.b_c = nn.Parameter(torch.zeros(hidden_size))\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        for param in self.parameters():\n",
    "            if param.dim() > 1:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "\n",
    "        # 需要初始化隐藏状态和细胞状态\n",
    "        h = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
    "        c = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
    "        y_list = []\n",
    "        for i in range(seq_len):\n",
    "            forget_gate = self.sigmoid(torch.matmul(x[:, i, :], self.w_f) +\n",
    "                                       torch.matmul(h, self.u_f) + self.b_f)\n",
    "            # (batch_siz,hidden_size)\n",
    "            input_gate = self.sigmoid(torch.matmul(x[:, i, :], self.w_i) +\n",
    "                                      torch.matmul(h, self.u_i) + self.b_i)\n",
    "            output_gate = self.sigmoid(torch.matmul(x[:, i, :], self.w_o) +\n",
    "                                       torch.matmul(h, self.u_o) + self.b_o)\n",
    "            # 这里可以看到各个门的运作方式。\n",
    "            # 三个门均通过hadamard积作用在每一个维度上。\n",
    "            c = forget_gate * c + input_gate * self.tanh(torch.matmul(x[:, i, :], self.w_c) +\n",
    "                                                         torch.matmul(h, self.u_c) + self.b_c)\n",
    "            h = output_gate * self.tanh(c)\n",
    "            y_list.append(h)\n",
    "        return torch.stack(y_list, dim=1), (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78308c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_LSTM(nn. Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gates = nn.Linear(input_size + hidden_size, hidden_size * 4)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn. Tanh()\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, output_size)\n",
    "        )\n",
    "        for param in self.parameters():\n",
    "            if param.dim() > 1:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "        h, c = (torch.zeros(batch_size, self.hidden_size).to(x.device) for _ in range(2))\n",
    "        y_list = []\n",
    "        for i in range(seq_len):\n",
    "            forget_gate, input_gate, output_gate, candidate_cell = \\\n",
    "                self.gates(torch.cat([x[:, i, :], h], dim=-1)).chunk(4, -1)\n",
    "            forget_gate, input_gate, output_gate = (self.sigmoid(g)\n",
    "                                                    for g in (forget_gate, input_gate, output_gate))\n",
    "            c = forget_gate * c + input_gate * self.tanh(candidate_cell)\n",
    "            h = output_gate * self.tanh(c)\n",
    "            y_list.append(self.output(h))\n",
    "        return torch.stack(y_list, dim=1), (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cec6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "TrafficData = TrafficDataset()\n",
    "train_set, val_set, test_set = TrafficData.construct_set()\n",
    "batch_size = 256\n",
    "train_loader = data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
    "val_loader = data.DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=0, drop_last=True)\n",
    "test_loader = data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0, drop_last=True)\n",
    "input_size = train_set.X.shape[-1]\n",
    "hidden_size = 6\n",
    "output_size = 1\n",
    "seq_len = 12\n",
    "lr = 0.0001\n",
    "epochs = 30\n",
    "loss_func = nn.MSELoss()\n",
    "my_lstm = My_LSTM(input_size, hidden_size, output_size).to(device)\n",
    "optimizer = torch.optim.Adam(my_lstm.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d703fb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse, mean_absolute_error as mae\n",
    "def mape(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    non_zero_index = (y_true > 0)\n",
    "    y_true = y_true[non_zero_index]\n",
    "    y_pred = y_pred[non_zero_index]\n",
    "\n",
    "    mape = np.abs((y_true - y_pred) / y_true)\n",
    "    mape[np.isinf(mape)] = 0\n",
    "    return np.mean(mape) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051e8af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "train_loss_lst, val_loss_lst, train_score_lst, val_score_lst, stop_epoch = train(my_lstm, train_loader, val_loader, test_loader,\n",
    "                                                       loss_func, TrafficData.denormalize, optimizer, epochs,\n",
    "                                                       early_stop=10, device=device, output_model=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c76499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(stop_epoch, train_loss_lst, val_loss_lst, y_label='Loss')\n",
    "plot_metric(train_score_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cabf2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数对比\n",
    "hidden_size = 16 # 对比hidden_size\n",
    "my_lstm = My_LSTM(input_size, hidden_size, output_size).to(device)\n",
    "optimizer = torch.optim.Adam(my_lstm.parameters(), lr)\n",
    "train_loss_lst, val_loss_lst, train_score_lst, val_score_lst, stop_epoch = train(my_lstm, train_loader, val_loader, test_loader,\n",
    "                                                       loss_func, TrafficData.denormalize, optimizer, epochs,\n",
    "                                                       early_stop=10, device=device, output_model=None)\n",
    "lr = 0.001 # 对比lr\n",
    "hidden_size = 6 \n",
    "my_lstm = My_LSTM(input_size, hidden_size, output_size).to(device)\n",
    "optimizer = torch.optim.Adam(my_lstm.parameters(), lr)\n",
    "train_loss_lst, val_loss_lst, train_score_lst, val_score_lst, stop_epoch = train(my_lstm, train_loader, val_loader, test_loader,\n",
    "                                                       loss_func, TrafficData.denormalize, optimizer, epochs,\n",
    "                                                       early_stop=10, device=device, output_model=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02897416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_size = train_set.X.shape[-1]\n",
    "hidden_size = 6\n",
    "output_size = 1\n",
    "seq_len = 12\n",
    "lr = 0.0001\n",
    "epochs = 30\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "nn_lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=1, batch_first=True).to(device)\n",
    "out_linear = nn.Sequential(nn.Linear(hidden_size, 1), nn.LeakyReLU()).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(list(nn_lstm.parameters()) + list(out_linear.parameters()), lr)\n",
    "\n",
    "train_loss_lst, val_loss_lst, train_score_lst, val_score_lst, stop_epoch = train(nn_lstm, train_loader, val_loader, test_loader,\n",
    "                                                       loss_func, TrafficData.denormalize, optimizer, epochs,\n",
    "                                                       early_stop=10, device=device, output_model=out_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9940da",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(stop_epoch, train_loss_lst, val_loss_lst, y_label='Loss')\n",
    "plot_metric(train_score_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea35a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数对比\n",
    "hidden_size = 16 # 对比hidden_size\n",
    "nn_lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=1, batch_first=True).to(device)\n",
    "out_linear = nn.Sequential(nn.Linear(hidden_size, 1), nn.LeakyReLU()).to(device)\n",
    "optimizer = torch.optim.Adam(list(nn_lstm.parameters()) + list(out_linear.parameters()), lr)\n",
    "train_loss_lst, val_loss_lst, train_score_lst, val_score_lst, stop_epoch = train(nn_lstm, train_loader, val_loader, test_loader,\n",
    "                                                       loss_func, TrafficData.denormalize, optimizer, epochs,\n",
    "                                                       early_stop=10, device=device, output_model=out_linear)\n",
    "hidden_size = 6\n",
    "lr = 0.001 #对比lr\n",
    "nn_lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=1, batch_first=True).to(device)\n",
    "out_linear = nn.Sequential(nn.Linear(hidden_size, 1), nn.LeakyReLU()).to(device)\n",
    "optimizer = torch.optim.Adam(list(nn_lstm.parameters()) + list(out_linear.parameters()), lr)\n",
    "train_loss_lst, val_loss_lst, train_score_lst, val_score_lst, stop_epoch = train(nn_lstm, train_loader, val_loader, test_loader,\n",
    "                                                       loss_func, TrafficData.denormalize, optimizer, epochs,\n",
    "                                                       early_stop=10, device=device, output_model=out_linear)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
